\documentclass{article}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{bm}
\usepackage{color}
\numberwithin{equation}{section}
\let\vec\bm
\newcommand{\qev}[1]{\langle #1 \rangle} % Quantum mechanical average.
\newcommand{\eav}[1]{\{ #1 \}}           % Ensemble average.
\DeclareMathOperator{\Tr}{Tr}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\numberwithin{thm}{section}

\theoremstyle{plain}
\newtheorem{prop}{Proposition}
\numberwithin{prop}{section}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\numberwithin{defn}{section}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}

\newtheorem*{cor}{Corollary}

\title{The Theory of Angular Momentum}
\author{Amey Joshi}
\date{24-Jan-2021}
\begin{document}
\maketitle
\abstract{These notes are based on chapter 3 of Sakurai's Modern Quantum 
Mechanics \cite{sakurai2011modern}.}
\section{Notation}\label{s1}
We use the mathematicians' notation in this article. The complex conjugate of
$z$ is denoted by $\bar{z}$ and the adjoint of an operator $A$ by $A^\ast$.
However, the inner product is defined with the property
\begin{equation}\label{s1e1}
(x,\alpha y)=\alpha(x, y)\;\forall x, y \in \mathcal{H}, \alpha \in \mathbf{C},
\end{equation}
where $\mathcal{H}$ is a Hilbert space.

The states of a quantum system are assumed to be members of a Hilbert space
$\mathcal{H}$. If $x \in \mathcal{H}$ then there exists a linear functional
$x^\ast \in \mathcal{H}^\ast$, the dual space of $\mathcal{H}$ such that
\begin{equation}\label{s1e2}
x^\ast(y) = (x, y), \forall y \in \mathcal{H}.
\end{equation}
Thus, $x$ corresponds to $|x\rangle$ and $x^\ast$ to $\langle x|$. The adjoint
of an operator $A$ is defined as
\begin{equation}\label{s1e3}
(x, A(y)) = (A^\ast(x), y).
\end{equation}
If $\{e_1, \ldots\, e_n\}$ for a basis of $\mathcal{H}$ then
\begin{equation}\label{s1e4}
\left(\sum_{i=1}^n e_i e_i^\ast\right)(x) = \sum_{i=1}^n e_i e_i^\ast(x) = 
\sum_{i=1}^n e_i (e_i, x) = x
\end{equation}
as a result
\begin{equation}\label{s1e5}
\sum_{i=1}^n e_i e_i^\ast = I_n,
\end{equation}
$I_n$ being the identity operator.

\section{The non-commutativity of rotations}\label{s2}
It is well-known that a body initially at a point $\vec{x}$ when first 
translated by $\vec{t}_1$ and then by $\vec{t}_2$ reaches the same position if 
we had first translated it by $\vec{t}_2$ and then by $\vec{t}_1$. This is 
because the result of two translation is an addition of vectors, a commutative 
operation.

Rotations cannot be described by vectors. If a vector $\vec{x} =
[x_1, x_2, x_3]^T$ is rotated then its components after rotation $[x_1^\prime, 
x_2^\prime, x_3^\prime]^T$ are related to its original components by 
\begin{equation}\label{s2e1}
\begin{bmatrix}x_1^\prime \\ x_2^\prime \\ x_3^\prime\end{bmatrix} = 
R\begin{bmatrix}x_1 \\ x_2 \\ x_3\end{bmatrix} 
\end{equation}
If $R_1$ and $R_2$ are the matrices representing two rotations and if they 
happen one after the other then their combined effect is represented by either
$R_2R_1$ or $R_1R_2$. Unlike addition of vectors, the multiplication of
matrices is \emph{not} commutative.

We shall follow the convention of interpreting a rotation as the act of moving
a vector keeping the coordinate systems untouched. With the convention, a
rotation of a vector by and angle $\phi$ about the three axes is represented
by
\begin{eqnarray}
R_x(\phi) &=& \begin{bmatrix}1 & 0 & 0 \\
0 & \cos\phi & -\sin\phi \\
0 & \sin\phi & \cos\phi
\end{bmatrix} \label{s2e2} \\
R_y(\phi) &=& \begin{bmatrix}\cos\phi & 0 & \sin\phi \\
0 & 1 & 0 \\
-\sin\phi & 0 & \cos\phi
\end{bmatrix} \label{s2e3} \\
R_z(\phi) &=& \begin{bmatrix}\cos\phi & -\sin\phi & 0 \\
\sin\phi & \cos\phi & 0 \\
0 & 0 & 1
\end{bmatrix} \label{s2e4}
\end{eqnarray}
Note that the negative sign of $\sin\phi$ appears in the $y$ position for $R_x$,
 $z$ position for $R_y$ and $x$ position for $R_z$.

If $\phi$ is infinitesimally small then we can write the above equations as
\begin{eqnarray}
R_x(\phi) &=& \begin{bmatrix}1 & 0 & 0 \\
0 & 1 - \phi^2/2 & -\phi \\
0 & \phi & 1 - \phi^2/2
\end{bmatrix} \label{s2e5} \\
R_y(\phi) &=& \begin{bmatrix}1 - \phi^2/2 & 0 & \phi \\
0 & 1 & 0 \\
-\phi & 0 & 1 - \phi^2/2
\end{bmatrix} \label{s2e6} \\
R_z(\phi) &=& \begin{bmatrix}1 - \phi^2/2 & -\phi & 0 \\
\phi & 1 - \phi^2/2 & 0 \\
0 & 0 & 1
\end{bmatrix} \label{s2e7}
\end{eqnarray}
We can easily verify that, when $\phi$ is infinitesimally small,
\begin{equation}\label{s2e8}
R_xR_y - R_yR_x = \begin{bmatrix}0 & -\phi^2 & 0 \\
\phi^2 & 0 & 0 \\
0 & 0 & 0 \end{bmatrix} = R_z(\phi^2) - I,
\end{equation}
where we ignored terms of $O(\phi^3)$ and above. In fact, if we ignore terms
of $O(\phi^2)$ and above then 
\begin{equation}\label{s2e9}
R_xR_y - R_yR_x = 0 \text{ up to } O(\phi).
\end{equation}
Thus, even if finite rotations do not commute, infinitesimal ones do. Can we 
express infinitesimal rotations as vectors?

\section{Infinitesimal rotations as vectors}\label{s3}
We notice that the matrix $R_z(\phi^2)$ is anti-symmetric. We will now argue
that every infinitisimal rotation is represented by an anti-symmetric matrix.
If $R$ is one such then we can write its effect on a vector $\vec{x}$ as
\begin{equation}\label{s3e1}
\vec{x}^\prime = (1 + R)\vec{x}.
\end{equation}
It is clear that, up to first order, the inverse of the transformation $1 + R$
is $1 - R$. We also know that rotation matrices are always orthogonal, that is
their transpose is their inverse. Therefore, for infinitesimal rotations, we
have \cite{wolfram1}
\begin{equation}\label{s3e2}
(1 + R)^T = (1 - R) \Rightarrow R^T = -R,
\end{equation}
that is, $R$ is anti-symmetric. An anti-symmetric matrix has only three
independent entries and is therefore isomorphic to a vector. One can represent
a general infinitesimal rotation as
\begin{equation}\label{s3e3}
R = \begin{bmatrix}0 & d\Omega_3 & -d\Omega_2 \\
-d\Omega_3 & 0 & d\Omega_1 \\
d\Omega_2 & -d\Omega_1 & 0
\end{bmatrix}
\end{equation}
so that 
\begin{equation}\label{s3e4}
\begin{bmatrix}x_1^\prime \\ x_2^\prime \\ x_3^\prime \end{bmatrix} = 
\begin{bmatrix}0 & d\Omega_3 & -d\Omega_2 \\
-d\Omega_3 & 0 & d\Omega_1 \\
d\Omega_2 & -d\Omega_1 & 0
\end{bmatrix}\begin{bmatrix}x_1 \\ x_2 \\ x_3 \end{bmatrix}
= \begin{bmatrix}
x_2d\Omega_3 - x_3d\Omega_2 \\
x_3d\Omega_1 - x_1d\Omega_3 \\
x_2d\Omega_2 - x_2d\Omega_1
\end{bmatrix}.
\end{equation}
We can write this equation succintly as
\begin{equation}\label{s3e5}
x_i^\prime = \epsilon_{ijk}x_jd\Omega_k
\end{equation}
which allows us to consider $[d\Omega_1, d\Omega_2, d\Omega_3]^T$ as a vector.
We can write equation \eqref{s3e6} as
\begin{equation}\label{s3e6}
\vec{x}^\prime = \vec{x} \wedge d\vec{\Omega}.
\end{equation}

An operator representing an infinitesimal rotation can be written as
\begin{equation}\label{s3e7}
D_k(\delta\phi) = 1 - i\frac{J_k}{\hslash}\delta\phi.
\end{equation}
If $J_k$ is hermitian then $D_k$ is unitary. If $\psi^\prime = D_k\psi$ then
$(\psi^\prime, \psi^\prime) = (D_k\psi, D_k\psi) = (\psi, D_k^\ast D_k \psi)
= (\psi, \psi)$, which is as it should be for a rotation opertor. The operator
for a finite rotation can be written as
\begin{equation}\label{s3e8}
D_k(\phi) = \lim_{n \rightarrow \infty} D_k\left(\frac{\phi}{n}\right)^n =
\lim_{n \rightarrow \infty}\left(1-i\frac{J_k}{\hslash}\frac{\phi}{n}\right)^n
= \exp\left(-\frac{iJ_k\phi}{\hslash}\right).
\end{equation}
We now observe that if $\varepsilon$ is an infinitesimal quantity then
\begin{eqnarray}
D_x(\varepsilon) &=& 1 - \frac{iJ_x\varepsilon}{\hslash} - 
\frac{J_x^2 \varepsilon^2}{\hslash^2} - \ldots \label{s3e9} \\
D_y(\varepsilon) &=& 1 - \frac{iJ_y\varepsilon}{\hslash} - 
\frac{J_y^2 \varepsilon^2}{\hslash^2} - \ldots \label{s3e10} 
\end{eqnarray}
so that
\begin{equation}\label{s3e11}
D_x(\varepsilon)D_y(\varepsilon) - D_y(\varepsilon)D_x(\varepsilon) =
-\frac{1}{\hslash}(J_xJ_y-J_yJ_x)\epsilon^2+O(\epsilon^3)
\end{equation}
From equation \eqref{s2e8}, the right hand side is expected to be
\begin{equation}\label{s3e12}
D_z(\epsilon^2) - 1 = -\frac{iJ_z}{\hslash}\epsilon^2
\end{equation}
so that we have
\begin{equation}\label{s3e13}
-(J_xJ_y - J_yJ_x) = -i\hslash J_z
\end{equation}
which is same as
\begin{equation}\label{s3e14}
[J_x, J_y] = i\hslash J_z.
\end{equation}
We can similarly show that
\begin{eqnarray}
[J_y, J_z] &=& i\hslash J_x \label{s3e15} \\
{}[J_z, J_x] &=& i\hslash J_y \label{s3e16}
\end{eqnarray}
The commutations relations of equations \eqref{s3e14}, \eqref{s3e15}
and \eqref{s3e16} depend only on:
\begin{enumerate}
\item definition \eqref{s3e8} of rotation operator and
\item expectation \eqref{s2e8} from the commutation of infinitesimal
rotations.
\end{enumerate}
In particular, we have nowhere used the classical definiton of the
angular momentum as $\vec{x} \wedge \vec{p}$.

\section{An overview of spin-1/2 systems}\label{s4}
The states $x_+$, $x_-$ are defined in such a way that
\begin{equation}\label{s4e1}
S_zx_{\pm} = \pm\frac{\hslash}{2}x_\pm.
\end{equation}
In the basis $\{x_+, x_-\}$ the three spin operators are defined as
\begin{eqnarray}
S_x &=& \frac{\hslash}{2}(x_+ x_-^\ast + x_- x_+^\ast) \label{s4e2} \\
S_y &=& -\frac{i\hslash}{2}(x_+ x_-^\ast + x_- x_+^\ast) \label{s4e3} \\
S_z &=& \frac{\hslash}{2}(x_+ x_+^\ast + x_- x_-^\ast) \label{s4e4}
\end{eqnarray}
From these equations, we can confirm that
\begin{eqnarray}
S_z x_+ &=& \frac{\hslash}{2}\left( x_+ x_+^\ast(x_+) + x_-x_-^\ast(x_+)\right)
\nonumber \\
 &=& \frac{\hslash}{2}\left(x_+(x_+, x_+) + x_-(x_-, x_+)\right) \nonumber \\
 &=& \frac{\hslash}{2}x_+. \label{s4e5}
\end{eqnarray}
Now consider a spin-1/2 system in a state $x$ and let it be subjected to the
operator $D_z(\phi)$ to give another state $y$. That is
\begin{equation}\label{s4e6}
y = D_z(\phi)(x).
\end{equation}
Since $\phi$ is fixed, we will use $D_z$ instead of $D_z(\phi)$ for sake of
brevity. We will now compute $\qev{S_x}$ after rotation. Thus,
\begin{eqnarray}
\qev{S_x} &=& (y, S_x(y)) \nonumber \\
 &=& (D_z(x), S_x(D_z(x))) \nonumber \\
 &=& (x, D_z^\ast S_x D_z (x)) \label{s4e7}
\end{eqnarray}
To make further progress, we will find out what $D_z^\ast S_xD_z$ is. To that
end, we use equation \eqref{s4e2} to get
\begin{equation}\label{s4e8}
D_z^\ast S_x D_z = e^{iS_z\phi/\hslash}\frac{\hslash}{2}(x_+x_-^\ast + 
x_-x_+^\ast)e^{-iS_z\phi/\hslash}
\end{equation}
Now,
\begin{equation}\label{s4e9}
e^{iS_z\phi/\hslash}x_+ = 
\sum_{k \ge 0}\left(\frac{i\phi}{\hslash}\right)^kS_z^k x_+
\end{equation}
Using equation \eqref{s4e1}
\begin{equation}\label{s4e10}
e^{iS_z\phi/\hslash}x_+ = 
\sum_{k \ge 0}\left(\frac{i\phi}{2}\right)^k x_+ = e^{i\phi/2}x_+.
\end{equation}
Similarly,
\begin{equation}\label{s4e11}
e^{-iS_z\phi/\hslash}x_- = e^{i\phi/2}x_-.
\end{equation}
The adjoints of equations \eqref{s4e9} and \eqref{s4e11} (explained at the
end of the section in equation \eqref{s4e34}) are
\begin{eqnarray}
x_+^\ast e^{-iS_z\phi/\hslash} &=& e^{-i\phi/2}x_+^\ast \label{s4e12} \\
x_-^\ast e^{-iS_z\phi/\hslash} &=& e^{i\phi/2}x_-^\ast \label{s4e13}
\end{eqnarray}
Using equations \eqref{s4e9} to \eqref{s4e11} in \eqref{s4e8} we get
\begin{eqnarray}
D_z^\ast S_x D_z &=& \frac{\hslash}{2}\left(e^{i\phi/2}x_+x_-^\ast e^{i\phi/2}
+ e^{-i\phi/2}x_-x_+^\ast e^{-i\phi/2}\right) \nonumber \\
 &=& x_+x_-^\ast e^{i\phi} + x_-x_+^\ast e^{-i\phi} \nonumber \\
 &=& (x_+x_-^\ast + x_-x_+^\ast)\cos\phi + i(x_+x_-^\ast - x_-x_+^\ast)\sin\phi
\label{s4e14}
\end{eqnarray}
Using equations \eqref{s4e3} and \eqref{s4e4} we get
\begin{equation}\label{s4e15}
D_z^\ast S_x D_z = S_x\cos\phi - S_y\sin\phi
\end{equation}
so that equation \eqref{s4e6} becomes
\begin{equation}\label{s4e16}
\qev{S_x}_y = (x, S_x\cos\phi - S_y\sin\phi, x) = \qev{S_x}_x\cos\phi -
\qev{S_y}_x\sin\phi,
\end{equation}
where the subscript to the expectation value indicates the state with respect
to which the expectation was calculated. We can similarly derive the analogous 
relations
\begin{eqnarray}
\qev{S_y}_y &=& \qev{S_x}_x\cos\phi + \qev{S_y}_x\sin\phi \label{s4e17} \\
\qev{S_z}_y &=& \qev{S_z}_y. \label{s4e18}
\end{eqnarray}
Equations \eqref{s4e16} to \eqref{s4e17} can be written as
\begin{equation}\label{s4e19}
\begin{bmatrix}\qev{S_x}_y \\ \qev{S_y}_y \\ \qev{S_z}_y\end{bmatrix}
= \begin{bmatrix}\cos\phi & -\sin\phi & 0 \\
\sin\phi & \cos\phi & 0 \\
0 & 0 & 1\end{bmatrix}
\begin{bmatrix}\qev{S_x}_x \\ \qev{S_y}_x \\ \qev{S_z}_x\end{bmatrix} =
R_z(\phi)\begin{bmatrix}\qev{S_x}_x \\ \qev{S_y}_x \\ \qev{S_z}_x\end{bmatrix}
\end{equation}
Thus, the expected values of the spin operators in the rotated state behave 
like vectors in $\mathbf{R}^3$.

In equation \eqref{s4e8} we assumed that 
\[
D_z(\phi) = \exp\left(-\frac{iS_z\phi}{\hslash}\right)
\]
and we used the representation of spin operators given by equations \eqref{s4e2}
to \eqref{s4e4} to get \eqref{s4e19}. Could we have drawn the same conclusion
for a general angular momentum operator? That is, will we get \eqref{s4e19}
if
\begin{equation}\label{s4e20}
D_z(\phi) = \exp\left(-\frac{iJ_z\phi}{\hslash}\right)?
\end{equation}
We will now demonstrate that we can indeed get \eqref{s4e19} based solely on
the commutation relations of the angular momentum components. In order to 
proceed we need Baker-Campbell-Hausdorff formula
\begin{equation}\label{s4e21}
e^{iG\lambda}Ae^{-iG\lambda} = 
\sum_{k \ge 0}\left(\frac{i^k\lambda^k}{k!}\right)[G, A]_k,
\end{equation}
where
\begin{equation}\label{s4e22}
[G, A]_k = \begin{cases}
A & \text{ if } k = 0 \\
[G, [G, A]_{k-1}] & \text{ if } k > 0.
\end{cases}
\end{equation}
In particular,
\begin{eqnarray}
[J_z, J_x]_0 &=& J_x \label{s4e23} \\
{}[J_z, J_x]_1 &=& i\hslash J_y \label{s4e24} \\
{}[J_z, J_x]_2 &=& (i\hslash)^2 (-J_x) \label{s4e25} \\
{}[J_z, J_x]_3 &=& -(i\hslash)^3 J_y \label{s4e26} \\
\vdots &=& \vdots \nonumber
\end{eqnarray}
so that
\[
e^{iJ_z\phi/\hslash}J_xe^{-iJ_z\phi/\hslash} = J_x + \frac{i\phi}{\hslash}
(i\hslash J_y) + \left(\frac{i\phi}{\hslash}\right)^2(-i\hslash)^2(-J_x)
+ \left(\frac{i\phi}{\hslash}\right)^3(i\hslash)^3(-J_y) + \ldots.
\]
Getting together the real and the imaginary parts together, we get
\[
e^{iJ_z\phi/\hslash}J_xe^{-iJ_z\phi/\hslash} = J_x\left(1 - \frac{\phi^2}{2!}
+ \cdots\right) - J_y\left(\phi - \frac{\phi^3}{3!} + \cdots\right)
\]
or
\begin{equation}\label{s4e27}
e^{iJ_z\phi/\hslash}J_xe^{-iJ_z\phi/\hslash} = J_x\cos\phi - J_y\sin\phi.
\end{equation}
We can similarly show that
\begin{eqnarray}
e^{iJ_z\phi/\hslash}J_ye^{-iJ_z\phi/\hslash} &=& J_x\sin\phi + J_y\cos\phi 
\label{s4e28} \\
e^{iJ_z\phi/\hslash}J_ze^{-iJ_z\phi/\hslash} &=& J_z \label{s4e29}
\end{eqnarray}

An arbitrary state of a spin-1/2 system can be expressed in terms of the
base states $\{x_+, x_-\}$ as
\begin{equation}\label{s4e30}
x = (x_+x_+^\ast + x_-x_-^\ast)x = x_+(x_+, x) + x_-(x_-, x)
\end{equation}
so that using equations \eqref{s4e10} and \eqref{s4e11} we have
\begin{equation}\label{s4e31}
e^{iS_z\phi/\hslash}x = e^{i\phi/2}(x_+(x_+,x)+x_-(x_-,x))=e^{i\phi/2}x.
\end{equation}
In particular, if $\phi = 2\pi$ we get the strange result
\begin{equation}\label{s4e32}
e^{2\pi i S_z/\hslash}x = -x.
\end{equation}
Thus, rotating a spin-1/2 state by $2\pi$ reverses its sign!

Before closing this section, we will explain how to compute the adjoint of
the expression $\exp(iS_z\hslash)x_+$. Note that the expression is a member
of the Hilbert space, say $y$, that is
\begin{equation}\label{s4e33}
y = \exp\left(\frac{iS_z}{\hslash}\right)x_+.
\end{equation}
Consider the expression $(y, z) = y^\ast(z)$. Now,
\begin{eqnarray*}
(y, z) &=& \left(\exp\left(\frac{iS_z}{\hslash}\right)x_+, z\right) \\
 &=& \left(x_+, \exp\left(-\frac{iS_z}{\hslash}\right)(z)\right) \\
 &=&  x_+^\ast\left(\exp\left(-\frac{iS_z}{\hslash}\right)(z)\right) \\
 &=&  x_+^\ast\exp\left(-\frac{iS_z}{\hslash}(z)\right)
\end{eqnarray*}
But this expression, we know, is $y^\ast(z)$. Therefore,
\begin{equation}\label{s4e34}
y^\ast = x_+^\ast\exp\left(-\frac{iS_z}{\hslash}\right).
\end{equation}

\section{Pauli matrices}\label{s5}
We will now compute the matrix elements of the spin operators defined by
equations \eqref{s4e2} to \eqref{s4e4}. As the state space of spin-1/2 systems
is two dimensional, the matrix representation of $S_z$ is
\begin{equation}\label{s5e1}
S_z = \begin{bmatrix} (x_+, S_z x_+) & (x_+, S_z x_-) \\
(x_-, S_z x_+) & (x_-, S_z x_-) 
\end{bmatrix} = \frac{\hslash}{2}\begin{bmatrix} 1 & 0 \\ 0 & -1\end{bmatrix}
\end{equation}
We can similarly show that
\begin{eqnarray}
S_x &=& \frac{\hslash}{2}\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix}
\label{s5e2} \\
S_y &=& \frac{\hslash}{2}\begin{bmatrix}0 & i \\ -i & 0 \end{bmatrix}
\label{s5e3}
\end{eqnarray}
The three matrices
\begin{eqnarray}
\sigma_x &=& \begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix} \label{s5e4} \\
\sigma_y &=& \begin{bmatrix}0 & i \\ -i & 0\end{bmatrix} \label{s5e5} \\
\sigma_z &=& \begin{bmatrix}1 & 0 \\ 0 & -1\end{bmatrix} \label{s5e6}
\end{eqnarray}
are called \emph{Pauli matrices}. Their main properties are
\begin{eqnarray}
\Tr(\sigma_i) &=& 0 \label{s5e7} \\
\det(\sigma_i) &=& -1 \label{s5e8} \\
\sigma_i^\ast &=& \sigma_i \label{s5e9} \\
\{\sigma_i, \sigma_j\} &=& 2\delta_{ij} \label{s5e10} \\
{}[\sigma_i, \sigma_j] &=& 2i\hslash\epsilon_{ijk}\sigma_k \label{s5e11} \\
\sigma_i^2 &=& I_2 \label{s5e12}
\end{eqnarray}

The three spin operators $S_x, S_y, S_z$ can be considered as components of
a vector operator $\vec{S}$. Likewise, the three Pauli matrices can be 
considered to be components of the vector operator $\vec{\sigma}$. It is
easy to confirm that
\begin{equation}\label{s5e13}
\vec{\sigma}\cdot\vec{a} = \begin{bmatrix}a_z & a_x + ia_y \\ a_x -ia_y & -a_z
\end{bmatrix}
\end{equation}
and
\begin{equation}\label{s5e14}
(\vec{\sigma}\cdot\vec{a})^2 = a^2 I_2,
\end{equation}
where $\vec{a} \in \mathbf{R}^3$ is an arbitrary vector. Since
\begin{equation}\label{s5e15}
\sigma_i\sigma_j = \frac{\{\sigma_i, \sigma_j\} + [\sigma_i, \sigma_j]}{2},
\end{equation}
we have
\begin{eqnarray}
(\vec{\sigma}\cdot\vec{a})(\vec{\sigma}\cdot\vec{b}) &=&
\sum_j \sigma_j a_j \sum_k \sigma_k b_k \nonumber \\
 &=& \sum_{j,k} \sigma_j\sigma_k a_j b_k \nonumber \\
 &=& \sum_{j,k}\frac{\{\sigma_i, \sigma_j\} + [\sigma_i, \sigma_j]}{2}a_jb_k
\nonumber \\
 &=& \sum_{j,k}(\delta_{jk} + i\epsilon_{jkl}\sigma_l)a_jb_k \nonumber \\
 &=& \vec{a}\cdot\vec{b} + i\vec{\sigma}\cdot(\vec{a} \wedge \vec{b}) 
\label{s5e16}.
\end{eqnarray}

From equation \eqref{s3e7} we can write the general rotation operator as
\begin{equation}\label{s5e17}
D(\phi) = \exp\left(-i\frac{\vec{J}\cdot\vec{n}\phi}{\hslash}\right).
\end{equation}
In the case of spin-1/2 systems, $\vec{J} = \vec{S}$ and since
\begin{equation}\label{s5e18}
\vec{S} = \frac{\hslash}{2}\vec{\sigma},
\end{equation}
the general rotation operator is
\begin{equation}\label{s5e19}
D(\phi) = \exp\left(-i\frac{\vec{\sigma}\cdot\vec{n}}{2}\phi\right).
\end{equation}
From equation \eqref{s5e10}
\[
(\vec{\sigma}\cdot\vec{n})^2 = \left(\sum_{i=1}^3\sigma_i n_i\right)^2 = 
\sum_{i,j=1}^3\sigma_i\sigma_jn_i n_j = \sum_{i=1}^3\sigma_i^2n_i^2 = 
\sum_{i=1}^3n_i^2 = 1
\]
so that we can write the general relation
\begin{equation}\label{s5e20}
(\vec{\sigma}\cdot\vec{n})^n = \begin{cases}
1 & \text{ if } n \text{ is even,} \\
\vec{\sigma}\cdot\vec{n} & \text{ if } n \text{ is odd.}
\end{cases}
\end{equation}
Therefore,
\begin{eqnarray}
D(\phi) &=& \sum_{k \ge 0}(-1)^k\left(\frac{\phi}{2}\right)^{2k}\frac{1}{(2k)!}
 + \nonumber \\
 & & -i\vec{\sigma}\cdot\vec{n}
 \sum_{k \ge 0}(-1)^k\left(\frac{\phi}{2}\right)^{2k+1}\frac{1}{(2k+1)!} 
 \nonumber \\
 &=& \cos\left(\frac{\phi}{2}\right) - i\vec{\sigma}\cdot\vec{n}
     \sin\left(\frac{\phi}{2}\right), \label{s5e21}
\end{eqnarray}
where it is understood that an appropriate idendity operator multiplies the
cosine term. We can express this result in matrix form as
\begin{equation}\label{s5e22}
D(\phi) = \begin{bmatrix}
\cos\left(\frac{\phi}{2}\right) -in_z\sin\left(\frac{\phi}{2}\right) &
(-in_x -n_y)\sin\left(\frac{\phi}{2}\right) \\
(-in_x + n_y)\sin\left(\frac{\phi}{2}\right) &
\sin\left(\frac{\phi}{2}\right) + in_z\cos\left(\frac{\phi}{2}\right)
\end{bmatrix}
\end{equation}
We notice, once again, that
\begin{equation}\label{s5e23}
D(2\pi) = -D.
\end{equation}

\section{Euler angles}\label{s6}
Any rotation can be described by the angle of rotation, $\phi$, and the axis
around which the rotation happens. The axis itself can be expressed by the
normal vector $\vec{n}$. A unit vector can be described by its polar and
azimuthal angles alone. Thus, three angles suffice to completely describe any
rotation.

Euler angles provide another way to specify the three angles. Consider a body
with three axes $x^\prime, y^\prime, z^\prime$ embedded in it. They are called
the body axes because they move as the body moves. The space axes, on the other
hand, stay fixed. The initial position of the body is chosen so that the two
sets of axes coincide. An arbitrary rotation can be described by three steps:
\begin{enumerate}
\item Rotate the body about the $z$ axis by an angle $\alpha$;
\item Rotate the body about the $y^\prime$ axis by an angle $\beta$ and
\item Rotate the body about the $z^\prime$ axis by an angle $\gamma$.
\end{enumerate}
Mathematically, the rotation is described as
\begin{equation}\label{s6e1}
R(\alpha, \beta, \gamma) = R_{z^\prime}(\gamma)R_{y^\prime}(\beta)R_z(\alpha)
\end{equation}
and the three angles $\alpha, \beta, \gamma$ are called the \emph{Euler angles}.

It would be more convenient if we can express the second and the third steps
in terms of rotations about the space axes. We can do so because of the 
relations
\begin{eqnarray}
R_{y^\prime}(\beta) &=& R_z(\alpha)R_y(\beta)R_z(-\alpha) \label{s6e2}\\
R_{z^\prime}(\gamma) &=& R_{y^\prime}(\beta)R_z(\gamma)R_{y^\prime}(-\beta).
\label{s6e3}
\end{eqnarray}
Substituting these equations in \eqref{s6e1} we get
\begin{eqnarray}
R(\alpha, \beta, \gamma) &=& R_{z^\prime}(\gamma)R_{y^\prime}(\beta)R_z(\alpha)
\nonumber \\
 &=& R_{y^\prime}(\beta)R_z(\gamma)R_{y^\prime}(-\beta)
     R_{y^\prime}(\beta)R_z(\alpha) \nonumber \\
 &=& R_{y^\prime}(\beta)R_z(\gamma)R_z(\alpha) \nonumber \\
 &=& R_z(\alpha)R_y(\beta)R_z(-\alpha)R_z(\gamma)R_z(\alpha) \nonumber \\
 &=& R_z(\alpha)R_y(\beta)R_z(-\alpha + \gamma + \alpha) \nonumber \\
 &=& R_z(\alpha)R_y(\beta)R_z(\gamma) \label{s6e4}
\end{eqnarray}

Translating equation \eqref{s6e4} to quantum mechanical language we get
\begin{equation}\label{s6e5}
D(\alpha,\beta,\gamma) = D_z(\alpha)D_y(\beta)D_z(\gamma),
\end{equation}
which is equivalent to
\begin{equation}\label{s6e6}
D(\alpha,\beta,\gamma) = \exp\left(-i\frac{J_z\alpha}{\hslash}\right)
\exp\left(-i\frac{J_y\beta}{\hslash}\right)
\exp\left(-i\frac{J_z\gamma}{\hslash}\right).
\end{equation}
When applied to a spin-1/2 system it becomes
\begin{equation}\label{s6e7}
D(\alpha,\beta,\gamma) = e^{-i\sigma_z\alpha/2}e^{-i\sigma_y\beta/2}
e^{-i\sigma_z\gamma/2}.
\end{equation}
Since $\sigma_z$ is a diagonal matrix, the matrix representation of the first
and the third factors on rhs of equation \eqref{s6e7} are easy to evaluate.
They are
\begin{eqnarray}
e^{-i\sigma_z\alpha/2} &=& \begin{bmatrix}e^{-i\alpha/2} & 0 \\
0 & e^{i\alpha/2}\end{bmatrix} \label{s6e8} \\
e^{-i\sigma_z\gamma/2} &=& \begin{bmatrix}e^{-i\gamma/2} & 0 \\
0 & e^{i\gamma/2}\end{bmatrix} \label{s6e9}
\end{eqnarray}
The second factor can be computed as
\[
e^{-i\sigma_y\beta/2} = \sum_{k\ge 0}\left(-\frac{i\sigma_y\beta}{2}\right)^k
\frac{1}{k!}.
\]
Since $\sigma_y^2 = I_2$, we get
\[
e^{-i\sigma_y\beta/2} = \sum_{k\ge 0}(-1)^k\left(\frac{\beta}{2}\right)^{2k}
\frac{1}{(2k)!} + 
i\sigma_y\sum_{k\ge 0}(-1)^k\left(\frac{\beta}{2}\right)^{2k+1}\frac{1}{(2k+1)!}
\]
or
\begin{equation}\label{s6e10}
e^{-i\sigma_y\beta/2} = \cos\left(\frac{\beta}{2}\right) + i\sigma_y
\sin\left(\frac{\beta}{2}\right).
\end{equation}
In matrix form,
\begin{equation}\label{s6e11}
e^{-i\sigma_y\beta/2} = \begin{bmatrix}
\cos\left(\frac{\beta}{2}\right) & -\sin\left(\frac{\beta}{2}\right) \\
\sin\left(\frac{\beta}{2}\right) & \cos\left(\frac{\beta}{2}\right)
\end{bmatrix}
\end{equation}
From equations \eqref{s6e7}, \eqref{s6e8}, \eqref{s6e9} and \eqref{s6e11}
we get the matrix representation
\begin{equation}\label{s6e12}
D(\alpha,\beta,\gamma) = \begin{bmatrix}
e^{-i(\alpha+\gamma)/2}\cos\left(\frac{\beta}{2}\right) & 
-e^{-i(\alpha-\gamma)/2}\sin\left(\frac{\beta}{2}\right) \\
e^{i(\alpha-\gamma)/2}\sin\left(\frac{\beta}{2}\right) & 
e^{i(\alpha+\gamma)/2}\cos\left(\frac{\beta}{2}\right)
\end{bmatrix}
\end{equation}

\section{Pure states}\label{s7}
The state $x_+$ is the one whose spin points `upwards' while the state $x_-$
is the onw whose spin points `downwards'. It is customary to treat `upwards'
to be along the positive $z$ axis and downwards to be along the negative
$z$ axis. Where does the spin of the state
\begin{equation}\label{s7e1}
x = c_+x_+ + c_-x_-
\end{equation}
point?

Let it point to along the direction specified by the normal $\vec{n}$ specified
by its azimuthal angle $\alpha$ and polar angle $\beta$. That is,
\begin{equation}\label{s7e2}
\vec{n} = \sin\beta\cos\alpha\vec{e}_x + \sin\beta\sin\alpha\vec{e}_y +
\cos\beta\vec{e}_z
\end{equation}
so that the matrix representation of $\vec{\sigma}\cdot\vec{n}$ is
\begin{equation}\label{s7e3}
\begin{bmatrix}
\cos\beta & \sin\beta e^{i\alpha} \\
\sin\beta e^{-i\alpha} & -\cos\beta
\end{bmatrix}.
\end{equation}
If $x$ is the eigenstate of $\vec{\sigma}\cdot\vec{n}$ then
\begin{equation}\label{s7e4}
\vec{\sigma}\cdot\vec{n}x = \lambda x,
\end{equation}
where $\lambda$ is the eigenvalue. It is easy to confirm that $\lambda = \pm 1$.
If $x = [x_1, x_2]^T$ then we further conclude that $x_2 = \pm x_1 e^{-i\alpha}
\tan(\beta/2)$. From this, we readily conclude that
\begin{equation}\label{s7e5}
\frac{c_-}{c_+} = \pm e^{-i\alpha}\tan\left(\frac{\beta}{2}\right).
\end{equation}
Thus, the state $x$ given by \eqref{s7e1} has its spin pointing along the
normal \eqref{s7e2} where the angles $\alpha$ and $\beta$ are determined by
equation \eqref{s7e5}.

\section{Mixed states}\label{s8}
The key point of the previous section is that the states $x_+, x_-$ and $x$ all 
have spins pointing to a definite direction. That is why they are called \emph{
pure states}. If we have a Stern-Gerlach apparatus pointing along $\vec{e}_z$ 
then it will give a reading $1$ with probabilities $1, 0$ and $|c_+|^2$ for 
states $x_+ , x_-$ and $x$. If we have an ensemble of states some of which have 
spins pointing upwards and the rest pointing downwards then we need a different 
way of descrbing it. Such a system is said to be in a \emph{mixed state}.

In general, if an ensemble is prepared from pure states $x_1, x_2, \ldots, x_M$
with proportions $w_1, w_2, \ldots, w_M$ with the restriction 
\begin{eqnarray}
w_i &\ge& 0 \;\forall i \label{s8e1} \\
\sum_{i=1}^M w_i &=& 1 \label{s8e2}
\end{eqnarray}
then the state of the system is represented by
\begin{equation}\label{s8e3}
x = \sum_{i=1}^M w_i x_i.
\end{equation}
Such a state is called the \emph{mixed state}. The number of terms in the sum
is not limited by the dimensions of the system.

If we measure an observabe $A$ on such a system then the expected value of the
measurement is
\begin{equation}\label{s8e4}
\eav{A} = \sum_{i=1}^Mw_i(x_i, A(x_i)) = \sum_{i=1}^Mw_i\qev{A}_i.
\end{equation}
The quantity $\eav{A}$ is called the \emph{ensemble average} in contrast with 
the quantum mechanical expectation value $\qev{A}_i$. The subscript $i$ to the
quantum mechanical average indicates the state in which it was taken. Let us 
express the state $x_i$ in terms of a basis $e_1, \ldots, e_N$ so that
\begin{equation}\label{s8e5}
x_i = \sum_{j=1}^N \alpha_j^{(i)} e_j.
\end{equation}
If $e_1, \ldots, e_N$ are an orthonormal basis then
\begin{equation}\label{s8e6}
\alpha_j^{(i)} = (x_i, e_j).
\end{equation}
Substituting this equation in \eqref{s8e4} we get
\begin{eqnarray*}
\eav{A} &=& \sum_{i=1}^Mw_i\left(\sum_{j=1}^N \alpha_j^{(i)} e_j, 
     A\left(\sum_{k=1}^N \alpha_k^{(i)} e_k\right)\right) \\
 &=& \sum_{i=1}^M w_i \sum_{j,k=1}^N\bar{\alpha}_j^{(i)}\alpha_k^{(i)}
     (e_j, A(e_k)) \\
 &=& \sum_{i=1}^M\sum_{j,k=1}^Nw_i(e_j, x_i)(x_i,e_k)(e_j, A(e_k)) \\
 &=& \sum_{i=1}^M\sum_{j,k=1}^Nw_i(x_i,e_k)(e_j, x_i)(e_j, A(e_k)) \\
 &=& \sum_{i=1}^M\sum_{j,k=1}^Nw_i\left(e_j, (x_i,e_k)x_i\right)(e_j,A(e_k))
\end{eqnarray*}
The manipulation in the last equation is possible because $(x_i, e_k)$ is a
number. Continuing further
\begin{equation}\label{s8e7}
\eav{A} = \sum_{i=1}^M\sum_{j,k=1}^Nw_i
     \left(e_j, x_i(x_i,e_k)\right)(e_j,A(e_k)) 
\end{equation}
The quantity
\begin{equation}\label{s8e8}
\left(e_j, \sum_{i=1}^M w_ix_ix_i^\ast(e_k))\right)
\end{equation}
is independent of the observable $A$. It depends solely on the composition of
the mixed state given by \eqref{s8e3}. The operator
\begin{equation}\label{s8e9}
\rho = \sum_{i=1}^M w_ix_ix_i^\ast
\end{equation}
is called the \emph{density operator}. We can write \eqref{s8e8} in terms of
the density operator as
\begin{equation}\label{s8e10}
\eav{A} = \sum_{j,k=1}^N (e_j, \rho(e_k))(e_j, A(e_k)).
\end{equation}
We can simplify this expression as
\begin{eqnarray}
\eav{A} &=& \sum_{j,k=1}^N((e_j, \rho(e_k))e_j, A(e_k)) \nonumber \\
 &=& \sum_{j,k=1}^N(e_j^\ast(\rho(e_k))e_j, A(e_k)) \nonumber \\
 &=& \sum_{k=1}^N\left(\left(\sum_{j=1}^N e_j e_j^\ast (\rho(e_k))\right), 
     A(e_k)\right) \label{s8e11} 
\end{eqnarray}
From the completeness relation, we know that
\begin{equation}\label{s8e12}
\sum_{j=1}^N e_j e_j^\ast = I,
\end{equation}
the identity operator so that equation \eqref{s8e11} becomes
\begin{equation}\label{s8e13}
\eav{A} = \sum_{k=1}^N (\rho(e_k), A(e_k)) = \sum_{k=1}^N(e_k, \rho(A(e_k)))
\end{equation}
because $\rho$ is Hermitian (proved in equation \eqref{s8e15}. We can write 
$\rho(A(e_k))$ as $(\rho A)(e_k)$ so that 
\begin{equation}\label{s8e14}
\eav{A} = \sum_{k=1}^N(e_k, (\rho A)(e_k)) = \Tr(\rho A).
\end{equation}

We note the following properties of the density operator:
\begin{enumerate}
\item $\rho$ is Hermitian. Consider
\begin{equation}\label{s8e15}
\rho^\ast = \sum_{i=1}^Mw_i(x_ix_i^\ast)^\ast = 
\sum_{i=1}^Mw_i x_i^{\ast\ast}x_i^\ast = \sum_{i=1}^Mw_i x_ix_i^\ast = \rho.
\end{equation}
\item $\rho$ has unit trace.
\begin{eqnarray}
\Tr(\rho) &=& \sum_{k=1}^N(e_k, \rho(e_k)) \nonumber \\
 &=&\sum_{k=1}^N\left(e_k, \sum_{i \ge 1}w_i x_ix_i^\ast(e_k)\right)\nonumber\\
 &=&\sum_{i=1}^M\sum_{k=1}^Nw_i(e_k, x_ix_i^\ast(e_k)) \nonumber \\
 &=&\sum_{i=1}^M\sum_{k=1}^N w_i(e_k, x_i(x_i, e_k)) \nonumber \\
 &=&\sum_{i=1}^M\sum_{k=1}^N w_i(x_i, e_k)(e_k, x_i) \nonumber \\
 &=&\sum_{i=1}^M\sum_{k=1}^N w_i |(x_i, e_k)|^2 \label{s8e16}
\end{eqnarray}
From equations \eqref{s8e5} and \eqref{s8e6} we know that $(x_i, e_k) = 
\alpha_k^{(i)}$ and 
\begin{equation}\label{s8e17}
\sum_{k=1}^N |\alpha_k^{(i)}|^2 = 1
\end{equation}
so that
\begin{equation}\label{s8e18}
\Tr(\rho) = \sum_{i=1}^Mw_i = 1.
\end{equation}

\item $\rho$ is positive semi-definite. Consider an arbitrary state 
\begin{equation}\label{s8e19}
y = \sum_{j=1}^N\beta_j e_j
\end{equation}
for which we compute
\begin{eqnarray}
(y, \rho(y)) &=& \sum_{i=1}^Mw_i(y, x_ix_i^\ast(y)) \nonumber \\
 &=& \sum_{i=1}^Mw_i\sum_{j,k=1}^N\bar{\beta}_j\beta_k(e_j, x_ix_i^\ast(e_k))
\nonumber \\
 &=& \sum_{i=1}^Mw_i\sum_{j,k=1}^N\bar{\beta}_j\beta_k(x_i, e_k)(e_j, x_i).
\nonumber
\end{eqnarray}
From equation \eqref{s8e5}
\begin{eqnarray}
(y,\rho(y)) &=& \sum_{i=1}^Mw_i\sum_{j,k=1}^N\bar{\beta}_j\beta_k
\sum_{l,m=1}^N \bar{\alpha}_l^{(i)}\alpha_m^{(i)}(e_l, e_k)(e_j, e_m) 
\nonumber \\
 &=& \sum_{i=1}^Mw_i\sum_{j,k=1}^N\bar{\beta}_j\beta_k
\sum_{l,m=1}^N \bar{\alpha}_l^{(i)}\alpha_m^{(i)}\delta_{lk}\delta{jm}
\nonumber \\
 &=& \sum_{i=1}^Mw_i\sum_{j,k=1}^N\bar{\beta}_j\beta_k\bar{\alpha}_k^{(i)}
\alpha_j^{(i)} \nonumber \\
 &=& \sum_{i=1}^Mw_i\sum_{j=1}^N\bar{\beta}_j\alpha_j^{(i)}
 \sum_{k=1}^N\beta_k\bar{\alpha}_k^{(i)} \nonumber \\
 &=& \sum_{i=1}^Mw_i\left|\sum_{j=1}^N\bar{\beta}_j\alpha_j^{(i)}\right|^2
 \ge 0 \label{s8e20}
\end{eqnarray}

\item If all members in an ensemble are in the state $x_j$, say, then the 
system is in a pure state and its density matrix is
\begin{equation}\label{s8e21}
\rho = x_j x_j^\ast
\end{equation}
because $w_i=1$ only for $i=j$ and zero for all others. In this case,
\begin{equation}\label{s8e22}
\rho^2 = (x_j x_j^\ast)(x_j x_j^\ast) = x_j x_j^\ast(x_j) x_j^\ast = 
x_j (x_j, x_j) x_j^\ast = x_j x_j^\ast = \rho.
\end{equation}
Thus, the density operator for a pure state is idempotent. The third step
follows from the second because a functional acts on the entity immediately to
its right.
\end{enumerate}

Refer to \cite[pages 179-180]{sakurai2011modern} for density matrices of 
spin-1/2 systems of various compositions.

We know that the time evolution of a state $x$ is governed by Schr\"{o}dinger
equation
\begin{equation}\label{s8e23}
i\hslash\frac{\partial x}{\partial t} = Hx
\end{equation}
whose formal solution is
\begin{equation}\label{s8e24}
x(t) = \exp\left(-\frac{iHt}{\hslash}\right)x(0).
\end{equation}
Therefore, the density operator evolves in time as
\[
\rho(t) = \sum_{i=1}^Mw_i \exp\left(-\frac{iHt}{\hslash}\right)x(0)x^\ast(0)
\exp\left(\frac{iHt}{\hslash}\right)
\]
which is same as
\begin{equation}\label{s8e25}
\rho(t) = \exp\left(-\frac{iHt}{\hslash}\right)\rho(0)
\exp\left(\frac{iHt}{\hslash}\right)
\end{equation}
because
\[
\rho(0) = \sum_{i=1}^M w_i x_i(0)x_i^\ast(0).
\]
We can also derive the differential equation of evolution of the density matrix.
\[
\frac{\partial\rho}{\partial t} = \sum_{i=1}^M w_i
\left(\frac{\partial x}{\partial t}x^\ast + x\frac{\partial x^\ast}{\partial t}\right)
\]
Taking the adjoint of equation \eqref{s8e23},
\[
-i\hslash\frac{\partial x^\ast}{\partial t} = x^\ast H^\ast = x^\ast H,
\]
as $H$ is assumed to be Hermitian. Therefore
\[
\frac{\partial\rho}{\partial t} = \sum_{i=1}^M w_i\left(
-\frac{iH}{\hslash}xx^\ast + xx^\ast\frac{iH}{\hslash}\right)
= -\frac{i}{\hslash}(H\rho - \rho H)
\]
or
\begin{equation}\label{s8e26}
i\hslash\frac{\partial\rho}{\partial t} = [H, \rho].
\end{equation}
This equation is analogous to the Lioville equation of classical stastical 
mechanics
\begin{equation}\label{s8e27}
\frac{\partial\rho}{\partial t} = \{H, \rho\}.
\end{equation}
Here $\{\cdot, \cdot\}$ denotes the Poisson bracket and \emph{not} the 
anti-commutator.

\section{Ladder operators}\label{s9}
We introduced the three angular momentum operators $J_x, J_y$ and $J_z$ in
section \ref{s3} and applied them to spin-1/2 systems in section \ref{s4}. We 
will study them for dimension $N > 2$ in this section.

Define the operator
\begin{equation}\label{s9e1}
J^2 = J_xJ_x + J_yJ_y + J_zJ_z.
\end{equation}
Consider the commutator
\begin{eqnarray}
[J^2, J_x] &=& [J_xJ_x, J_x] + [J_yJ_y, J_x] + [J_zJ_z, J_x] \nonumber \\
 &=& 0 + J_yJ_yJ_x - J_xJ_yJ_y + J_zJ_zJ_x - J_xJ_zJ_z \nonumber \\
 &=& J_yJ_yJ_x - J_yJ_xJ_y + J_yJ_xJ_y - J_xJ_yJ_y \nonumber \\
 & & J_zJ_zJ_x - J_zJ_xJ_z + J_zJ_xJ_z - J_xJ_zJ_z \nonumber \\
 &=& J_y[J_y, J_x] + [J_y, J_x]J_y + J_z[J_z, J_x] + [J_z, J_x]J_z \nonumber \\
 &=& -i\hslash J_yJ_z - i\hslash J_zJ_y + i\hslash J_zJ_y + i\hslash J_yJ_z
\nonumber  \\
 &=& 0 \label{s9e2}
\end{eqnarray}
By symmetry, we conclude that
\begin{eqnarray}
[J^2, J_y] &=& 0 \label{s9e3} \\
{}[J^2, J_z] &=& 0 \label{s9e4}
\end{eqnarray}

Since $J^2$ commutes with all of $\{J_x, J_y, J_z\}$ but the latter set of
operators do not commute among each other, we can find simultaneous eigenstates
of $J^2$ and one of $\{J_x, J_y, J_z\}$. By convention, we choose $J^2$ and
$J_z$.

Let $l$ and $m$ denote the eigenvalues of $J^2$ and $J_z$ so that we have
\begin{eqnarray}
J^2(x_{l,m}) &=& l x_{l, m} \label{s9e5} \\
J_z(x_{l,m}) &=& m x_{l, m} \label{s9e6} 
\end{eqnarray}
We also introduce two new operators
\begin{eqnarray}
J_+ &=& J_x + iJ_y \label{s9e7} \\
J_- &=& J_x - iJ_y \label{s9e8}
\end{eqnarray}
We observe that
\begin{eqnarray}
[J_+, J_-] &=& J_+J_- - J_-J_+ \nonumber \\
 &=& -iJ_xJ_y + iJ_yJ_x -iJ_xJ_y + i J_yJ_x \nonumber \\
 &=& -2i[J_x, J_y] \nonumber \\
 &=& 2\hslash J_z \label{s9e9}
\end{eqnarray}
and 
\begin{eqnarray}
[J_z, J_\pm] &=& J_zJ_\pm - J_\pm J_z \nonumber \\
 &=& J_zJ_x \pm iJ_zJ_y - J_xJ_z \mp iJ_yJ_z \nonumber \\
 &=& [J_z, J_x] \mp i[J_y,J_z] \nonumber \\
 &=& i\hslash J_y \mp (-\hslash J_x) \nonumber \\
 &=& i\hslash J_y \pm \hslash J_x \nonumber \\
 &=& \pm\hslash(J_x \pm iJ_y)  \nonumber \\
 &=& \pm\hslash J_\pm. \label{s9e10}
\end{eqnarray}
Further, since $J^2$ commutes with all components of $\vec{J}$,
\begin{equation}\label{s9e11}
[J^2, J_\pm] = 0.
\end{equation}
From equation \eqref{s9e9} we have, from equation \eqref{s9e10},
\[
J_zJ_\pm = [J_z, J_\pm] + J_\pm J_z = \pm\hslash J_\pm + J_\pm J_z
\]
so that
\begin{equation}\label{s9e12}
J_zJ_\pm x_{l,m} = \pm\hslash J_\pm x_{l,m} + J_\pm mx_{l,m}
= (m \pm \hslash)J_\pm x_{l, m}
\end{equation}
which suggests that the effect of $J_\pm$ on $x_{l,m}$ is to raise or lower
the eigenvalue of $J_z$ by $\hslash$. That is why $J_\pm$ are called the ladder
operators. On the other hand, owing to \eqref{s9e11},
\begin{equation}\label{s9e13}
J^2J_\pm(x_{l,m}) = J_\pm J^2(x_{l,m}) = lJ_\pm(x_{l, m}).
\end{equation}
The ladder operators have no effect on the eigenvalues of $J^2$. Equations 
\eqref{s9e12} and \eqref{s9e13} tell that $J_\pm x_{l,m}$ are simultaneous
eigenstates of $J^2$ and $J_z$ with eigenvalues $l$ and $m \pm \hslash$. 
Contrast this with equations \eqref{s9e5} and \eqref{s9e6} which tell that
$x_{l,m}$ are simultaneous eigenstates of $J^2$ and $J_z$ with eigenvalues
$l$ and $m$.

We now prove 
\begin{prop}\label{s9p1}
$l \ge m^2$ where $l$ and $m$ are defined by equations \eqref{s9e5} and
\eqref{s9e6}.
\end{prop}
\begin{proof}
From the definitions of ladder operators in equations \eqref{s9e7} and
\eqref{s9e8} we observe that $J_- = J_+^\ast$ and that
\begin{equation}\label{s9e14}
J_+J_- + J_-J_+ = 2(J_x^2 + J_y^2)
\end{equation}
or that
\begin{equation}\label{s9e15}
J^2 - J_z^2 = \frac{1}{2}(J_x^2 + J_y^2) = \frac{J_+J_+^\ast + J_-J_-^\ast}{2}
\end{equation}
from which we get
\begin{equation}\label{s9e16}
(x_{l,m}, (J^2 - J_z^2)(x_{l,m})) = 
\frac{1}{2}(x_{l,m}, J_+J_+^\ast x_{l,m}) + (x_{l,m}, J_-J_-^\ast x_{l,m}).
\end{equation}
Now $(x_{l,m}, J_+J_+^\ast x_{l,m}) = (J_+^\ast x_{l,m},J_+^\ast x_{l,m})\ge 0$.
Similarly $ (x_{l,m}, J_-J_-^\ast x_{l,m}) \ge 0$ so that
\begin{equation}\label{s9e17}
(x_{l,m}, (J^2 - J_z^2)(x_{l,m})) = (l - m^2) \ge 0.
\end{equation}
\end{proof}

\begin{prop}\label{s9p2}
If $\tilde{m}$ is the largest value of $m$ then 
\begin{equation}\label{s9e18}
l = \tilde{m}(\tilde{m} + \hslash).
\end{equation}
\end{prop}
\begin{proof}
From proposition \ref{s9p1} $\tilde{m}$ exists. Consider a state 
$x_{l,\tilde{m}}$. Since $\tilde{m}$ is the maximum value of the second
index $m$, we must have
\begin{equation}\label{s9e19}
J_+( x_{l,\tilde{m}}) = 0
\end{equation}
from which we conclude that $J_-J_+(x_{l,\tilde{m}}) = 0$ or
\begin{equation}
(J_x^2 + J_y^2 - \hslash J_z)(x_{l, \tilde{m}}) = 0 \Rightarrow
(J^2 - J_z^2 - \hslash J_z)(x_{l, \tilde{m}}) = 0
\end{equation}
or
\begin{equation}\label{s9e21}
(l^2 - \tilde{m}^2 - \hslash\tilde{m})x_{l, \tilde{m}} = 0.
\end{equation}
As $x_{l, \tilde{m}}$ is assumed to be non-null, we have 
$l = \tilde{m}(\tilde{m} + \hslash)$.
\end{proof}

Define
\begin{equation}\label{s9e22}
j = \frac{\tilde{m}}{\hslash}
\end{equation}
then we can write equation \eqref{s9e21} as
\begin{equation}\label{s9e23}
l = j(j + 1)\hslash^2.
\end{equation}
Thus, if $j$ is the maximum value of $m$ then $l = j(j+1)\hslash$ and $m$ can
take $2j+1$ values $-j\hslash, (-j + 1)\hslash, \ldots, j\hslash$. It is 
convenient to express equations \eqref{s9e5} and \eqref{s9e6} in terms of $j$
and $m$ as 
\begin{eqnarray}
J^2(x_{j,m}) &=& j(j + 1)\hslash^2 x_{j, m} \label{s9e24} \\
J_z(x_{j,m}) &=& m\hslash x_{j, m} \label{s9e25}
\end{eqnarray}
It is important to realise that equations \eqref{s9e24} and \eqref{s9e25}
depend only on the angular momentum commuation relations, which in turn depend
only on the properties of rotation. In particular, we have not used the 
classical analogue of $\vec{x} \wedge \vec{p}$.

\section{Addition of angular momenta}\label{s10}
Consider two angular momentum operators, $\vec{J}_1$ and $\vec{J}_2$, operating
on two different Hilbert spaces. Individually, their components obey the
commutation relations
\begin{eqnarray}
[J_{1i}, J_{1j}] &=& i\hslash \epsilon_{ijk}J_{1k} \label{s10e1} \\
{}[J_{2i}, J_{2j}] &=& i\hslash \epsilon_{ijk}J_{2k} \label{s10e2} 
\end{eqnarray}
An infinitesimal rotation of \emph{both} systems about an axix $\vec{n}$ 
by an infinitesimal angle $\delta\phi$ is
\begin{equation}\label{s10e3}
\left(1 - \frac{i\vec{J}_1\cdot\vec{n}\delta\phi}{\hslash}\right) \otimes
\left(1 - \frac{i\vec{J}_2\cdot\vec{n}\delta\phi}{\hslash}\right)
= 1 - \frac{i(\vec{J}_1 \otimes I_2 + I_1 \otimes \vec{J}_2).\vec{n}\delta\phi}
{\hslash}
\end{equation}
We define the total angular momentum operator
\begin{equation}\label{s10e4}
\vec{J} = (\vec{J}_1 \otimes I_2 + I_1 \otimes \vec{J}_2).
\end{equation}
Note that:
\begin{enumerate}
\item $\vec{J}_1$ is an operator on a Hilbert space $\mathcal{H}_1$, $\vec{J}_2$
is an operator on $\mathcal{H}_2$ and $\vec{J}$ on $\mathcal{H}_1 \otimes
\mathcal{H}_2$.
\item $I_1$ is the identity operator on a Hilbert space $\mathcal{H}_1$
and $I_2$ is the identity operator on $\mathcal{H}_2$.
\item The definition of \eqref{s10e5} is possible because both systems 
are rotated by the same angle $\delta\phi$ about the same axis $\vec{n}$.
\end{enumerate}

\begin{prop}\label{s10p1}
For the operator defined by equation \eqref{s10e4},
\begin{equation}\label{s10e5}
[J_i, J_j] = i\epsilon_{ijk}J_k.
\end{equation}
\end{prop}
\begin{proof}
Consider the state $x = x_1 \otimes x_2$ on which $[J_i, J_j]$ acts. Thus,
\begin{eqnarray*}
[J_i, J_j](x) &=& (J_iJ_j - J_jJ_i)(x) \\
 &=&(J_{1i}\otimes I_2+I_1\otimes J_{2i})(J_{1j}\otimes I_2+I_1\otimes J_{2j})
(x_1 \otimes x_2) - \\
 & &(J_{1j}\otimes I_2+I_1\otimes J_{2j})(J_{1i}\otimes I_2+I_1\otimes J_{2i}) 
(x_1 \otimes x_2) \\
 &=&(J_{1i}\otimes I_2+I_1\otimes J_{2i})(J_{1j}(x_1)\otimes x_2 + x_1 \otimes
J_{2j}(x_2)) - \\
 & &(J_{1j}\otimes I_2+I_1\otimes J_{2j})(J_{1i}(x_1)\otimes x_2 + x_1 \otimes
J_{2i}(x_2)) \\
 &=& J_{1i}J_{1j}(x_1) \otimes x_2+J_{1i}(x_1)\otimes J_{2j}(x_2)+J_{1j}(x_1)
\otimes J_{2i}(x_2) + \\
 & & x_1 \otimes J_{2i}J_{2j}(x_2) - \\
 & & J_{1j}J_{1i}(x_1) \otimes x_2-J_{1j}(x_1)\otimes J_{2i}(x_2)-J_{1i}(x_1)
\otimes J_{2j}(x_2) - \\
 & & x_1 \otimes J_{2j}J_{2i}(x_2)  \\
 &=& J_{1i}J_{1j}(x_1) \otimes x_2 + x_1 \otimes J_{2i}J_{2j}(x_2) - \\
 & & J_{1j}J_{1i}(x_1) \otimes x_2 - x_1 \otimes J_{2j}J_{2i}(x_2) \\
 &=& \{J_{1i}J_{1j} - J_{1j}J_{1i}\}(x_1) \otimes x_2 + 
x_1 \otimes \{J_{2i}J_{2j} - J_{2j}J_{2i}\}(x_2)
\end{eqnarray*}
Using the commutation relations of \eqref{s10e1} and \eqref{s10e2}
\begin{eqnarray*}
[J_i, J_j](x) &=& i\hslash\epsilon_{ijk}J_{1k}(x_1)\otimes x_2 + 
x_1 \otimes (i\hslash\epsilon_{ijk}J_{2k}(x_2) \\
 &=& i\hslash\epsilon_{ijk}J_k(x)
\end{eqnarray*}
\end{proof}

\begin{rem}
The operator $\vec{J}$ is indeed an angular momentum operator and can be
considered to be the angular momentum of the composite system.
\end{rem}

Note that, from equation \eqref{s10e4}
\begin{equation}\label{s10e6}
\vec{J}(x) = (\vec{J}_1 \otimes I_2 + I_1 \otimes \vec{J}_2)(x_1 \otimes x_2)
= \vec{J}_1(x_1) \otimes I_2 + I_1 \otimes \vec{J}_2(x_2).
\end{equation}
Moreover,
\begin{equation}\label{s10e7}
\vec{J}(x) \ne \vec{J}_1(x_1) \otimes \vec{J}_2(x_2).
\end{equation}
This last assertion can be easily checked with small matrices $A$ and $B$.

Sometimes we see the commutation relation
\begin{equation}\label{s10e8}
[J_{1i}, J_{2j}] = 0.
\end{equation}
It should be interpreted as
\begin{equation}\label{s10e9}
[J_{1i} \otimes I_2, I_1 \otimes J_{2j}] = 0.
\end{equation}
It is true because
\begin{eqnarray*}
[J_{1i} \otimes I_2, I_1 \otimes J_{2j}](x) &=&
(J_{1i} \otimes I_2)(I_1 \otimes J_{2j})(x) - 
(I_1 \otimes J_{2j})(J_{1i} \otimes I_2)(x) \\
&=& (J_{i1}\otimes I_2)(x_1 \otimes J_{2j}(x_2)) - (I_1\otimes J_{2j})
(J_{1i}(x_1) \otimes x_2) \\
&=& J_{1i}(x_1) \otimes J_{2j}(x_2) - J_{1i}(x_1) \otimes J_{2j}(x_2) \\
&=& 0
\end{eqnarray*}
The commutation relation
\begin{equation}\label{s10e10}
[J_1^2, J_2^2] = 0
\end{equation}
should also be interpreted as
\begin{equation}\label{s10e11}
[J_1^2 \otimes I_2, I_1 \otimes J_2^2] = 0
\end{equation}
and is proved exactly like the way we proved \eqref{s10e8}.

Now consider the operator
\begin{equation}\label{s10e12}
J^2 = (\vec{J} \otimes I_2 + I_1 \otimes\vec{J}_2)^2.
\end{equation}
We observe that
\begin{eqnarray*}
J^2(x) &=& (\vec{J}_1 \otimes I_2 + I_1 \otimes\vec{J}_2)
(\vec{J}_1(x_1) \otimes x_2 + x_1 \otimes \vec{J}_2(x_2) \\
&=& J_1^2(x_1) \otimes x_2 + \vec{J}_1(x_1) \otimes \vec{J}_2(x_2) + 
\vec{J}_1(x_1) \otimes \vec{J}_2(x_2) + x_1 \otimes J_2^2(x_2) \\
&=& J_1^2(x_1) \otimes x_2 + 2\vec{J}_1(x_1) \otimes \vec{J}_2(x_2) + 
x_1 \otimes J_2^2(x_2) \\
&=& (J_1^2 \otimes I_2 + 2\vec{J}_1 \otimes \vec{J}_2 + I_1 \otimes J_2^2)(x)
\end{eqnarray*}
so that
\begin{equation}\label{s10e13}
J^2 =(J_1^2 \otimes I_2 + 2\vec{J}_1 \otimes \vec{J}_2 + I_1 \otimes J_2^2).
\end{equation}

The components of $\vec{J}$ satisfy the commutation relations \eqref{s10e5},
because of which we have
\begin{equation}\label{s10e14}
[J^2, J_z] = 0.
\end{equation}
We will now prove a few more propositions.
\begin{prop}\label{s10p2}
\begin{eqnarray}
[J^2, J_1^2 \otimes I_2] &=& 0 \label{s10e15} \\
{}[J^2, I_1 \otimes J_2^2] &=& 0 \label{s10e16}
\end{eqnarray}
\end{prop}
\begin{proof}
From equation \eqref{s10e13}, we have
\begin{eqnarray*}
[J^2, J_1^2 \otimes I_2] &=&
[J_1^2 \otimes I_2 + 2\vec{J}_1 \otimes \vec{J}_2 + I_1 \otimes J_2^2, 
J_1^2 \otimes I_2] \\
&=& 0 + 2[\vec{J}_1 \otimes \vec{J}_2, J_1^2 \otimes I_2] + 0 \\
&=& 2\sum_{j,k}[J_{1j} \otimes J_{2k}, J_1^2 \otimes I_2] \\
&=& 0
\end{eqnarray*}
because $[J_{1j}, J_1^2] = 0$ for all $j$.
\end{proof}

\begin{prop}\label{s10p3}
\begin{eqnarray}
[J^2, J_{1i} \otimes I_2] &\ne 0\label{s10e17}  \\
{}[J^2, I_1 \otimes J_{2i}] &\ne 0\label{s10e18}
\end{eqnarray}
\end{prop}
\begin{proof}
From equation \eqref{s10e13},
\[
[J^2, J_{1i} \otimes I_2] = [J_1^2 \otimes I_2, J_{1i} \otimes I_2] + 
2[\vec{J}_1 \otimes \vec{J}_2, J_{1i} \otimes I_2] + 
[I_1 \otimes J_2^2, J_{1i} \otimes I_2].
\]
The first commutator on the rhs is zero because $[J_1^2, J_{1i}] = 0$ for 
all $i = 1, 2, 3$. The third commutator on the rhs is zero because of 
\eqref{s10e9}. Therefore,
\begin{equation}\label{s10e19}
[J^2, J_{1i} \otimes I_2] = 2\sum_{j,k}[J_{1j} \otimes J_{2k}, J_{1i} 
\otimes I_2].
\end{equation}
Consider
\begin{eqnarray*}
[J_{1j} \otimes J_{2k}, J_{1i} \otimes I_2](x) &=& 
(J_{1j} \otimes J_{2k})(J_{1i} \otimes I_2)(x) -  \\
& & (J_{1i} \otimes I_2)((J_{1j} \otimes J_{2k})(x)  \\
&=& (J_{1j} \otimes J_{2k})(J_{1i}(x_1) \otimes x_2) - \\
& & (J_{1i} \otimes I_2)(J_{1j}(x_1) \otimes J_{2k}(x_2) \\
&=& J_{1j}J_{1i}(x_1)\otimes J_{2k}(x_2)-J_{1i}J_{1j}(x_1)\otimes J_{2k}(x_2) \\
&=& [J_{1j}, J_{1i}](x_1) \otimes J_{2k}(x_2) \\
&=& i\hslash\epsilon_{jil}J_{1l}(x_1) \otimes J_{2k}(x_2)
\end{eqnarray*}
We will now explicity write the nine terms on the rhs of \eqref{s10e17}.
\begin{eqnarray*}
-\frac{i}{\hslash}[J^2, J_{1i} \otimes I_2](x) &=&  
 \epsilon_{1il}J_{1l}(x_1) \otimes J_{21}(x_2) + \\
& & \epsilon_{1il}J_{1l}(x_1) \otimes J_{22}(x_2) + \\
& & \epsilon_{1il}J_{1l}(x_1) \otimes J_{23}(x_2) + \\
& & \epsilon_{2il}J_{1l}(x_1) \otimes J_{21}(x_2) + \\
& & \epsilon_{2il}J_{1l}(x_1) \otimes J_{22}(x_2) + \\
& & \epsilon_{2il}J_{1l}(x_1) \otimes J_{23}(x_2) + \\
& & \epsilon_{3il}J_{1l}(x_1) \otimes J_{21}(x_2) + \\
& & \epsilon_{3il}J_{1l}(x_1) \otimes J_{22}(x_2) + \\
& & \epsilon_{3il}J_{1l}(x_1) \otimes J_{23}(x_2) 
\end{eqnarray*}
Note that each term on the rhs is really a summation over $l$. Writing the
terms of the sum, after taking into account the properties of the Levi-Civita
tensor, we get
\begin{eqnarray*}
-\frac{i}{\hslash}[J^2, J_{1i} \otimes I_2](x) &=&  
    \epsilon_{1i2}J_{12}(x_1) \otimes J_{21}(x_2) + 
    \epsilon_{1i3}J_{13}(x_1) \otimes J_{21}(x_2)\\
& & \epsilon_{1i2}J_{12}(x_1) \otimes J_{22}(x_2) + 
    \epsilon_{1i3}J_{13}(x_1) \otimes J_{22}(x_2)\\
& & \epsilon_{1i2}J_{12}(x_1) \otimes J_{23}(x_2) + 
    \epsilon_{1i3}J_{13}(x_1) \otimes J_{23}(x_2)\\
& & \epsilon_{2i1}J_{11}(x_1) \otimes J_{21}(x_2) + 
    \epsilon_{2i3}J_{13}(x_1) \otimes J_{21}(x_2) \\
& & \epsilon_{2i1}J_{11}(x_1) \otimes J_{22}(x_2) + 
    \epsilon_{2i3}J_{13}(x_1) \otimes J_{22}(x_2) \\
& & \epsilon_{2i1}J_{11}(x_1) \otimes J_{23}(x_2) + 
    \epsilon_{2i3}J_{13}(x_1) \otimes J_{23}(x_2)\\
& & \epsilon_{3i1}J_{11}(x_1) \otimes J_{21}(x_2) + 
    \epsilon_{3i2}J_{12}(x_1) \otimes J_{21}(x_2)\\
& & \epsilon_{3i1}J_{11}(x_1) \otimes J_{22}(x_2) + 
    \epsilon_{3i2}J_{12}(x_1) \otimes J_{22}(x_2)\\
& & \epsilon_{3i1}J_{11}(x_1) \otimes J_{23}(x_2) + 
    \epsilon_{3i2}J_{12}(x_1) \otimes J_{23}(x_2)
\end{eqnarray*}
Without loss of generality, choose $i=3$, so that the rhs simplifies to
\begin{eqnarray*}
-\frac{i}{\hslash}[J^2, J_{1i} \otimes I_2](x) &=&  
    \epsilon_{132}J_{12}(x_1) \otimes J_{21}(x_2) + 
 \epsilon_{132}J_{12}(x_1) \otimes J_{22}(x_2) + \\
& & \epsilon_{132}J_{12}(x_1) \otimes J_{23}(x_2) + 
 \epsilon_{231}J_{11}(x_1) \otimes J_{21}(x_2) + \\
& & \epsilon_{231}J_{11}(x_1) \otimes J_{22}(x_2) + 
 \epsilon_{231}J_{11}(x_1) \otimes J_{23}(x_2) \\
&\ne& 0.
\end{eqnarray*}
\end{proof}

From equations \eqref{s10e9} and \eqref{s10e11}, the operators $J_{1z} \otimes
I_2, I_1 \otimes J_{2z}, J_1^2 \otimes I_2$ and $I_1 \otimes J_2^2$ all 
commute with each other. From equations \eqref{s10e14}, \eqref{s10e15} and
\eqref{s10e16} we see that the operators $J^2, J_z, J_1^2 \otimes I_2$ and 
$I_1 \times J_2^2$ also commute with each other. Therefore, one can express
the eigenstates of the direct product of two systems as
\begin{enumerate}
\item Simultaneous eigenstates $x_{j_1,j_2;m_1,m_2}$ of $J_1^2 \otimes I_2, 
I_1 \otimes J_2^2, J_{1z} \otimes I_2, I_2 \otimes J_{2z}$ \emph{or}
\item Simultaneous eigenstates $y_{j_1,j_2;j,m}$ of $J^2, J_z, J_1^2 \otimes 
I_2, I_1 \otimes J_2^2$.
\end{enumerate} 

They have the following properties. Often times $j_1, j_2$ are understood
and suppressed from the symbols of the states. Thus, we denote $y_{j_1,j_2;j,m}$
as $y_{j,m}$ and $x_{j_1,j_2;m_1,m_2}$ as $x_{m_1,m_2}$.
\begin{eqnarray}
J_1^2(x_{m_1,m_2}) &=& j_1(j_1 + 1)\hslash^2 x_{m_1,m_2}
\label{s10e20} \\
J_2^2(x_{m_1,m_2}) &=& j_2(j_2 + 1)\hslash^2 x_{m_1,m_2}
\label{s10e21} \\
J_{1z}(x_{m_1,m_2}) &=& m_1\hslash x_{m_1,m_2}\label{s10e22} \\
J_{2z}(x_{m_1,m_2}) &=& m_2\hslash x_{m_1,m_2}\label{s10e23} \\
J^2(y_{j,m}) &=& j(j+1)\hslash^2 y_{j,m} \label{s10e24} \\
J_z(y_{j,m}) &=& m\hslash y_{j,m} \label{s10e25} \\
J_1^2(y_{j,m}) &=& j_1(j_1+1)\hslash^2 y_{j,m} \label{s10e26} \\
J_2^2(y_{j,m}) &=& j_2(j_2+1)\hslash^2 y_{j,m} \label{s10e27} 
\end{eqnarray}

Since both choices form a complete set of states, we have
\begin{equation}\label{s10e28}
y_{j,m} = \sum_{m_1, m_2}x_{m_1,m_2}x_{m_1,m_2}^\ast (y_{j,m})
\end{equation}
The quantities $x_{m_1,m_2}^\ast(y_{j,m})$ are called the \emph{Clebsch-Gordon coefficients}. They have the following properties.

\begin{prop}\label{s10p4}
$x_{m_1,m_2}^\ast(y_{j,m}) = 0$ unless $m = m_1 + m_2$.
\end{prop}
\begin{proof}
Since $J_z = J_{1z} \otimes I_2 + I_1 \otimes J_{2z}$ so that
\begin{eqnarray}
J_z y_{j,m} &=& my_{j,m} \label{s10e29} \\
(J_{1z}\otimes I_2)(y_{j,m}) &=& \sum_{m_1,m_2}m_1x_{m_1,m_2}x_{m_1,m_2}^\ast
(y_{j,m}) \label{s10e30} \\
(I_1\otimes J_{2z})(y_{j,m}) &=& \sum_{m_1,m_2}m_2x_{m_1,m_2}x_{m_1,m_2}^\ast
(y_{j,m}) \label{s10e31}
\end{eqnarray}
We also have
\begin{equation}\label{s10e32}
my_{j,m} = m\sum_{m_1,m_2}x_{m_1,m_2}x_{m_1,m_2}^\ast(y_{j,m}).
\end{equation}
Therefore,
\begin{equation}\label{s10e33}
(J_z - J_{1z} \otimes I_2 - I_1 \otimes J_{1z})(y_{j,m}) = 0
\end{equation}
so that
\begin{equation}\label{s10e34}
\sum_{m_1,m_2}(m - m_1 - m_2)x_{m_1,m_2}x_{m_1,m_2}^\ast(y_{j,m}) = 0
\end{equation}
If $x_{m_1,m_2}$ are non-trivial then either $x_{m_1,m_2}^\ast(y_{j,m}) = 0$
or $m = m_1 + m_2$.
\end{proof}

\begin{prop}\label{s10p5}
For given values of $j_1$ and $j_2$, the space spanned by $x_{m_1,m_2}$ is
of dimensions $(2j_1 + 1)(2j_2 + 1)$.
\end{prop}
\begin{proof}
For a given value of $j$ there are $2j+1$ values of $m$ ranging from $-j$ to
$+j$.
\end{proof}

\begin{prop}\label{s10p6}
$x_{m_1,m_2}^\ast(x_{j,m}) = 0$ unless $|j_1 - j_2| \le j \le j_1 + j_2$.
\end{prop}
\begin{proof}
Fix $j_1$ and $j_2$ and without loss of generality, let $j_1 \ge j_2$. The
maximum value of $m_1$ is $j_1$ and that of $m_2$ is $j_2$. The maximum value
of $m$ is $j_1 + j_2$. Thus the two states $x_{j_1, j_2}$ (in $x_{m_1,m_2}$
basis) is equivalent to $x_{j_1+j_2, j_1+j_2}$ (in $x_{j,m}$ basis). The minimum
value if $m_1$ is $-j_1$ and that of $m_2$ is $-j_2$ so that the least value
$m$ can take is $-j_1-j_2$. For this case, the two states $x_{-j_1,-j_2}$
and $x_{-j_1-j_2,-j_1-j_2}$ are equivalent.

The next highest value of $m$ is $j_1+j_2-1$. We can get it with $m_1 = j_1$ 
and $m_2 = j_2 - 1$ or $m_1 = j_1 - 1$ and $m_2 = j_2$. Thus, there are two 
states for this value of $m$. By the same reasoning it follows that there are
two states for $m = -j_1 - j_2 + 1$.

The third highest value of $m$ is $j_1 + j_2 - 2$. It can be achieved by
the states $x_{j_1, j_2 - 2}, x_{j_1 - 1, j_2 - 1}, x_{j_1 - 2, j_2}$. Similarly
the third lowest value of $m$ can be described by three states.

The degeneracy will be maximum when the term added to $j_1$ is the least, that
is $-j_2$. This is also the least value of $j$. Thus, when $j_1 \ge j_2$, $
j_1 - j_2 \le j \le j_1 + j_2$. If $j_1 \le j_2$, we will get $j_2 - j_1 \le
j \le j_1 + j_2$.
\end{proof}

\begin{prop}\label{s10p7}
The Clebsch-Gordon coefficients form a unitary matrix.
\end{prop}
\begin{proof}
From equation \eqref{s10e28}
\[
y_{j,m} = \sum_{m_1,m_2} x_{m_1,m_2}x^\ast_{m_1,m_2}(y_{j,m})
\]
so that
\begin{eqnarray*}
(y_{j^\prime,m^\prime}, y_{j,m}) &=& 
 \left(\sum_{m_1,m_2}x_{m_1,m_2}x_{m_1,m_2}^\ast(y_{j^\prime,m^\prime}),
 \sum_{n_1,n_2}x_{n_1,n_2}x_{n_1,n_2}^\ast(y_{j^\prime,m^\prime})\right) \\
 &=& \sum_{m_1,m_2,n_1,n_2}\overline{(x_{m_1,m_2}, y_{j^\prime,m^\prime})}
 (x_{n_1,n_2}, y_{j,m}) (x_{m_1,m_2}, x_{n_1,n_2}) \\
 &=& \sum_{m_1,m_2,n_1,n_2}\overline{(x_{m_1,m_2}, y_{j^\prime,m^\prime})}
 (x_{n_1,n_2}, y_{j,m}) \delta_{m_1,n_1}\delta_{m_2,n_2} \\
 &=& \sum_{m_1,m_2}(y_{j^\prime,m^\prime}, x_{m_1,m_2})(x_{m_1,m_2}, y_{j,m}) \\
 &=& \sum_{m_1,m_2} y_{j^\prime,m^\prime}^\ast(x_{m_1,m_2}) x^\ast_{m_1,m_2}
 (y_{j,m}).
\end{eqnarray*}
Since $y_{j,m}$ are an orthonormal basis we have
\[
\sum_{m_1,m_2} y_{j^\prime,m^\prime}^\ast(x_{m_1,m_2}) x^\ast_{m_1,m_2}(y_{j,m})
= \delta_{jj^\prime}\delta_{mm^\prime}.
\]
\end{proof}

\section{Recurrence relations for Clebsch-Gordon coefficients}\label{s11}
We will need expressions for the action of the ladder operators $J_\pm$ on the
state $x_{m_1,m_2}$ to develop the recurrence relations for the Clebsch-Gordon
coefficients. We will do so first for the states $x_{j,m}$. Since
\[
J_+ = J_x + iJ_y,
\]
we have
\[
J_+^\ast = J_x^\ast - iJ_y^\ast = J_x - iJ_y
\]
so that
\begin{equation}\label{s11e1}
J_+^\ast J_+ = J_x^2 + J_y^2 -i[J_x, J_y] = J^2 - J_z^2 + i(i\hslash J_z)
= J^2 - J_z^2 - \hslash J_z
\end{equation}
If 
\begin{equation}\label{s11e2}
J_+(x_{j,m}) = c_{j,m}x_{j,m+1}
\end{equation} 
then 
\begin{equation}\label{s11e3}
(x_{j,m}, J_+^\ast J_+(x_{j,m}) = (J_+(x_{j,m}), J_+(x_{j,m})) = 
|J_+(x_{j,m})|^2 = |c_{j,m}|^2,
\end{equation}
if we assume that $x_{j,m+1}$ is normalised. If we substitute \eqref{s11e1}
into the lhs of \eqref{s11e3} then we get
\begin{eqnarray}
(x_{j,m}, J_+^\ast J_+(x_{j,m})) &=& 
 (x_{j,m}, (j(j+1) - m^2 - m)x_{j,m})\hslash^2 \nonumber \\
 &=& (j(j+1) - m^2 - m)\hslash^2 \label{s11e4} 
\end{eqnarray}
so that we have
\begin{equation}\label{s11e5}
c_{j,m} = \sqrt{j(j+1)-m^2-m}\hslash.
\end{equation}
We choose $c_{j,m}$ to be positive by convention. That is its phase is $0$.
Thus, we have
\begin{eqnarray}
J_+(x_{j,m}) &=& \sqrt{j(j+1)-m^2-m}\hslash x_{j,m+1} \nonumber \\
 &=& \sqrt{(j-m)(j+m+1)}\hslash x_{j,m+1} \label{s11e6}
\end{eqnarray}
Similarly, we have
\begin{eqnarray}
J_-(x_{j,m}) &=& \sqrt{j(j+1)-m^2+m}\hslash x_{j,m-1} \nonumber \\
 &=& \sqrt{(j+m)(j-m+1)}\hslash x_{j,m-1} \label{s11e7}
\end{eqnarray}
The ladder operators on the space $\mathcal{H}_1 \otimes \mathcal{H}_2$ are
defined as
\begin{eqnarray}
J_+ &=& J_{1+} \otimes I_2 + I_1 \otimes J_{2+} \label{s11e8} \\
J_- &=& J_{1-} \otimes I_2 + I_1 \otimes J_{2-}. \label{s11e9}
\end{eqnarray}
Now,
\begin{equation}\label{s11e10}
x_{m_1,m_2} \equiv x_{j_1,m_1;j_2,m_2} = x_{j_1,m_1} \otimes x_{j_2,m_2}
\end{equation}
so that
\begin{eqnarray}
J_+(x_{m_1,m_2}) &=&
 J_{1+}(x_{j_1,m_1}) \otimes I_2 + I_1 \otimes J_{2+}(x_{j_2,m_2}) \nonumber \\
 &=& \sqrt{(j_1-m_1)(j_1+m_1+1)}\hslash x_{j_1,m_1+1} \otimes I_2 + \nonumber \\
 & &I_1 \otimes \sqrt{(j_2-m_2)(j_2+m_2+1)}\hslash x_{j_2,m_2+1}.\label{s11e11}
\end{eqnarray}
Similarly,
\begin{eqnarray}
J_-(x_{m_1,m_2})
 &=& \sqrt{(j_1+m_1)(j_1-m_1+1)}\hslash x_{j_1,m_1-1} \otimes I_2 + \nonumber \\
 & &I_1 \otimes \sqrt{(j_2+m_2)(j_2-m_2+1)}\hslash x_{j_2,m_2-1}.\label{s11e12}
\end{eqnarray}
Recall that the state $x_{j_1,m_1;j_2,m_2}$ is an abbreviation for $x_{j_1,m_1}
\otimes x_{j_2,m_2}$ and we have studied the effect of the ladder operators on
$x_{j,m}$. The ladder operators on $\mathcal{H}_1 \otimes \mathcal{H}_2$ are
also defined as
\begin{equation}\label{s11e13}
J_{\pm} = J_x \pm iJ_y
\end{equation}
where the operators $J_x$ and $J_y$ are defined on $\mathcal{H}_1 \otimes 
\mathcal{H}_2$. Therefore,
\begin{equation}\label{s11e14}
J\pm(x_{j_1,j_2;j,m}) \equiv J_\pm(x_{j,m}) = 
\sqrt{(j \mp m)(j \pm m + 1)}\hslash x_{j,m}.
\end{equation}
Recall the definition \eqref{s10e28} of the Clebsch-Gordon coefficients
\[
y_{j,m} = \sum_{m_1, m_2}x_{m_1,m_2}x_{m_1,m_2}^\ast (y_{j,m})
\]
so that
\begin{eqnarray*}
J_+(y_{j,m}) &=& \sum_{m_1,m_2}J_+(x_{m_1,m_2})x_{m_1,m_2}^\ast (y_{j,m})
\\
\sqrt{(j-m)(j+m+1)}\hslash y_{j,m+1}&=&\sum_{m_1,m_2}x_{m_1,m_2}^\ast (y_{j,m})
\\
 & &\left(\sqrt{j_1-m_1)(j_1+m_1+1)}\hslash x_{j_1,m_1+1} \otimes x_{j_2,m_2} 
+ \right.  \\
 & & \left. x_{j_1,m_1} \otimes \sqrt{(j_2-m_2)(j_2+m_2+1)}\hslash x_{j_2,m_2+1}\right)
\end{eqnarray*}
or
\begin{eqnarray}
\sqrt{(j-m)(j+m+1)}y_{j,m+1} &=& \sum_{m_1,m_2}x_{m_1,m_2}^\ast (y_{j,m}) 
\nonumber \\
 & & \left(\sqrt{j_1-m_1)(j_1+m_1+1)}x_{j_1,m_1+1} \otimes x_{j_2,m_2} + 
\right. \nonumber \\
 & & \left. x_{j_1,m_1} \otimes \sqrt{(j_2-m_2)(j_2+m_2+1)}x_{j_2,m_2+1}\right)
\nonumber \\
 & & \label{s11e15}
\end{eqnarray}
Similarly by considering $J_-(y_{j,m})$ we get
\begin{eqnarray}
\sqrt{(j+m)(j-m+1)}y_{j,m-1} &=& \sum_{m_1,m_2}x_{m_1,m_2}^\ast (y_{j,m}) 
\nonumber \\
 & & \left(\sqrt{j_1+m_1)(j_1-m_1+1)}x_{j_1,m_1-1} \otimes x_{j_2,m_2}+\right. 
\nonumber \\
 & & \left. x_{j_1,m_1} \otimes \sqrt{(j_2+m_2)(j_2-m_2+1)}x_{j_2,m_2-1}\right)
\nonumber \\
 & & \label{s11e16}
\end{eqnarray}
Recall that the Clebsch-Gordon coefficients are $x_{m_1,m_2}^\ast(y_{j,m}) = 
(x_{m_1,m_2}, y_{j,m})$. To get a recurrence relation for them, apply the 
functional $x_{j_1,j_2;m_1^\prime, m_2^\prime}^\ast \equiv x_{m_1^\prime, 
m_2^\prime}^\ast$ on equation \eqref{s11e15}. This is equivalent to taking an 
inner product of the equation with $x_{m_1^\prime, m_2^\prime}$. Thus,
\begin{equation}\label{s11e17}
\begin{split}
\sqrt{(j-m)(j+m+1)}x^\ast_{m_1^\prime, m_2^\prime}(y_{j,m+1}) =&  \\
 \sum_{m_1,m_2}x_{m_1,m_2}^\ast (y_{j,m}) 
 \left(\sqrt{j_1-m_1)(j_1+m_1+1)}x_{m_1^\prime, m_2^\prime}^\ast
(x_{j_1,m_1+1} \otimes x_{j_2,m_2}) + \right. & \\
  \left. \sqrt{(j_2-m_2)(j_2+m_2+1)}x_{m_1^\prime,m_2^\prime}^\ast
(x_{j_1,m_1} \otimes x_{j_2,m_2+1})\right)
\end{split}
\end{equation}
We now consider the expression
\begin{eqnarray}
x_{m_1^\prime, m_2^\prime}^\ast(x_{j_1,m_1+1} \otimes x_{j_2,m_2}) &=& 
x_{j_1, m_1^\prime}^\ast \otimes x_{j_2, m_2^\prime}^\ast
(x_{j_1,m_1+1} \otimes x_{j_2,m_2})
\nonumber \\
 &=& x_{j_1, m_1^\prime}^\ast(x_{j_1,m_1+1}) x_{j_2, m_2^\prime}^\ast(
x_{j_2,m_2}) \nonumber \\
 &=& \delta_{m_1^\prime, m_1+1} \delta_{m_2^\prime,m_2} \label{s11e18}
\end{eqnarray}
Similarly,
\begin{eqnarray}
x_{m_1^\prime,m_2^\prime}^\ast(x_{j_1,m_1} \otimes x_{j_2,m_2+1}) &=&
 x_{j_1,m_1^\prime}^\ast \otimes x_{j_2,m_2^\prime}^\ast
(x_{j_1,m_1} \otimes x_{j_2,m_2+1}) \nonumber \\
 &=& x_{j_1,m_1^\prime}^\ast(x_{j_1,m_1}) x_{j_2,m_2^\prime}^\ast(x_{j_2,m_2+1})
\nonumber \\
 &=& \delta_{m_1^\prime,m_1} \delta_{m_2^\prime,m_2+1} \label{s11e19}
\end{eqnarray}
Using equations \eqref{s11e18} and \eqref{s11e19} in \eqref{s11e17} we get
{\tiny
\begin{eqnarray}
\sqrt{(j-m)(j+m+1)}x_{m_1^\prime,m_2^\prime}^\ast(y_{j,m+1}) &=&
\sqrt{(j_1-m_1^\prime+1)(j_1+m_1^\prime)}x_{m_1^\prime-1,m_2^\prime}
     (y_{j,m}) + \nonumber \\
 & & \sqrt{(j_1-m_2^\prime+1)(j_2+m_2^\prime)}x_{m_1^\prime,m_2^\prime-1}
     (y_{j,m}) \nonumber \\
 & & \label{s11e20}
\end{eqnarray}}
Similarly, we have
{\tiny
\begin{eqnarray}
\sqrt{(j+m)(j-m+1)}x_{m_1^\prime,m_2^\prime}^\ast(y_{j,m+1}) &=&
\sqrt{(j_1+m_1^\prime+1)(j_1-m_1^\prime)}x_{m_1^\prime+1,m_2^\prime}
     (y_{j,m}) + \nonumber \\
 & & \sqrt{(j_1+m_2^\prime+1)(j_2-m_2^\prime)}x_{m_1^\prime,m_2^\prime+1}
     (y_{j,m}) \nonumber \\
 & & \label{s11e21}
\end{eqnarray}}
Equations \eqref{s11e20} and \eqref{s11e21} are the recurrence relations for
the Clebsch-Gordon coefficients.

\section{Schwinger's oscillator model}\label{s12}
Consider two uncoupled harmonic oscillators with creation and annihilation
operators $a_+, a_+^\ast$ and $a_-, a_-^\ast$ with the following properties
\begin{eqnarray}
[a_+, a_+^\ast] &=& 1 \label{s12e1} \\
{}[a_-, a_-^\ast] &=& 1 \label{s12e2} \\
{}[a_+, a_-^\ast] &=& 1 \label{s12e3} \\
{}[a_-, a_+^\ast] &=& 1 \label{s12e4} 
\end{eqnarray}
and with the `number' operators
\begin{eqnarray}
N_+ &=& a_+^\ast a_+ \label{s12e5} \\
N_- &=& a_-^\ast a_- \label{s12e6}
\end{eqnarray}
Clearly, $[N_+, N_-] = 0$ and therefore the two number operators have 
simultaneous eigenstates $x_{n_+,n_-}$ such that
\begin{eqnarray}
N_+(x_{n_+,n_-}) &=& n_+x_{n_+,n_-} \label{s12e7} \\
N_-(x_{n_+,n_-}) &=& n_-x_{n_+,n_-} \label{s12e8} 
\end{eqnarray}
We also have the usual properties
\begin{eqnarray}
a_+^\ast(x_{n_+, n_-}) &=& \sqrt{n_+ + 1} x_{n_+ + 1, n_-} \label{s12e9} \\
a_+(x_{n_+, n_-}) &=& \sqrt{n_+} x_{n_+, n_- - 1} \label{s12e10} \\
a_-^\ast(x_{n_+, n_-}) &=& \sqrt{n_- + 1} x_{n_+, n_- + 1} \label{s12e11} \\
a_-(x_{n_+, n_-}) &=& \sqrt{n_-} x_{n_+, n_- - 1} \label{s12e12}
\end{eqnarray}
Starting with the vacuum state $x_{0,0}$, defined by its properties
\begin{eqnarray}
a_+(x_{0,0}) &=& 0 \label{s12e13} \\
a_-(x_{0,0}) &=& 0 \label{s12e14} \\
a_+^\ast(x_{0,0}) &=& x_{1,0} \label{s12e15} \\
a_-^\ast(x_{0,0}) &=& x_{0,1} \label{s12e16}
\end{eqnarray}
we can obtain any state $x_{m,n}$ as
\begin{equation}\label{s12e17}
x_{m,n} = \frac{(a_+^\ast)^m (a_-^\ast)^m}{\sqrt{m!}\sqrt{n!}} x_{0,0}.
\end{equation}
Define the operators
\begin{eqnarray}
J_+ &=& \hslash a_+^\ast a_- \label{s12e18} \\
J_- &=& \hslash a_-^\ast a_+ \label{s12e19} \\
J_z &=& \frac{\hslash}{2}(N_+ - N_-) \label{s12e20}
\end{eqnarray}
Consider
\begin{eqnarray}
[J_z, J_+] &=& J_zJ_+ - J_+J_z \nonumber \\
 &=& \frac{\hslash^2}{2}(a_+^\ast a_+ a_+^\ast a_- - a_+^\ast a_- a_+^\ast a_+)
 - \frac{\hslash^2}{2}(a_-^\ast a_- a_+^\ast a_- - a_+^\ast a_- a_-^\ast a_-)
 \nonumber 	\\
 &=& \frac{\hslash^2}{2}(a_+^\ast a_+ a_+^\ast a_- - a_+^\ast a_+^\ast a_+a_-)
  - \frac{\hslash^2}{2}(a_+^\ast a_-^\ast a_- a_- - a_+^\ast a_- a_-^\ast a_-)
 \nonumber 	\\
 &=& \frac{\hslash^2}{2}a_+^\ast(a_+ a_+^\ast - a_+^\ast a_+)a_- - 
 \frac{\hslash^2}{2}a_+^\ast(a_-^\ast a_- - a_-a_-^\ast)a_- \nonumber \\
 &=& \hslash^2 a_+^\ast a_- \nonumber \\
 &=& \hslash J_+ \label{s12e21}
\end{eqnarray}
Similarly, we can show that
\begin{equation}\label{s12e22}
[J_z, J_-] = -\hslash J_-.
\end{equation}
Now consider
\begin{eqnarray}
[J_+, J_-] &=& \hslash(a_+^\ast a_- a_-^\ast a_+ - a_-^\ast a_+ a_+^\ast a_-)
	\nonumber \\
&=&\hslash(a_+^\ast a_+ + a_+^\ast a_-^\ast a_- a_+ - a_+ a_+^\ast a_-^\ast a_-)
 \nonumber \\
&=&\hslash(N_+ + [a_+^\ast, a_+]N_-) \nonumber \\
&=& 2\hslash J_z \label{s12e23}
\end{eqnarray}

Recall equation \eqref{s9e15} from which we infer that
\begin{equation}\label{s12e24}
J^2 = J_z^2 + \frac{J_x^2 + J_y^2}{2} = J_z^2 + \frac{J_+J_- + J_-J_+}{2}.
\end{equation}
In the present context, we define the $J^2$ operator using the extreme lhs and
extreme rhs of the above equation. That is, we define
\begin{equation}\label{s12e25}
J^2 = J_z^2 + \frac{J_+J_- + J_-J_+}{2}.
\end{equation}
We also define the total number operator
\begin{equation}\label{s12e26}
N = N_+ + N_- = a_+^\ast a_+ + a_-^\ast a_-.
\end{equation}
From equation \eqref{s12e25},
\begin{eqnarray}
J^2 &=& \frac{\hslash^2}{4}(N_+ - N_-)^2 + \frac{\hslash^2}{2}
\left(a_+^\ast a_-a_-^\ast a_+ + a_-^\ast a_+a_+^\ast a_-\right) \nonumber \\
&=& \frac{\hslash^2}{4}(N_+ - N_-)^2 + \frac{\hslash^2}{2}
\left(a_+^\ast a_+a_-a_-^\ast  + a_-^\ast a_-a_+a_+^\ast \right) \nonumber \\
&=& \frac{\hslash^2}{4}(N_+ - N_-)^2 + \frac{\hslash^2}{2}
\left(a_+^\ast a_+a_-^\ast a_- a_+^\ast a_+ + a_-^\ast a_-a_+^\ast a_+ + a_-^\ast a_-\right)
\nonumber \\
&=& \frac{\hslash^2}{4}\left((N_+ - N_-)^2 + 2(N_+N_- + N_+ + + N_-N_+ + N_-)
\right) \nonumber \\
&=& \frac{\hslash^2}{4}(N_+^2 - N_+N_- - N_-N_+ + N_-^2 + 2N_+N_- +2N_-N_+ + 
2N_+ + 2N_-) \nonumber \\
&=& \frac{\hslash^2}{4}\left((N_+ + N_-)^2 + 2(N_+ + N_-)\right) \nonumber \\
&=& \frac{\hslash^2}{4}(N^2 + 2N) \nonumber \\
&=& \frac{\hslash^2}{2}N\left(\frac{N}{2} + 1\right) \label{s12e27}
\end{eqnarray}
We also confirm that
\begin{eqnarray}
J_+(x_{n_+,n_-}) &=& \sqrt{(n_+ + 1)n_-}\hslash
x_{n_+ + 1, n_- - 1} \label{s12e28} \\
J_-(x_{n_+,n_-}) &=& \sqrt{n_+(n_- + 1)}\hslash
x_{n_+ - 1, n_- + 1} \label{s12e29} \\
J_z(x_{n_+,n_-}) &=& \frac{\hslash}{2}(n_+ - n_-)x_{n_+, n_-} \label{s12e30}
\end{eqnarray}

From the analysis so far we infer that
\begin{itemize}
\item There are two types of particles, `+' and `-'.
\item $a_+^\ast$ ($a_-^\ast$) creates one quantum of `+' (`-') particles. Their
adjoints destroy the particles.
\item $N_+$ ($N_-$) counts the number of `+' (`-') particles.
\item $J_+$ destroys a `-' particle and creates a `+' particle. Likewise, $J_-$
destroys a `+' particle and creates a `-' particle.  
\item $J_z$ counts the signed difference between the number of `+' and `-' 
particles multiplied by $\hslash/2$. $x_{n_+,n_-}$ is an eigenfunction of $J_z$.
\end{itemize}
We now introduce the numbers
\begin{eqnarray}
j &=& \frac{n_+ + n_-}{2} \label{s12e31} \\
m &=& \frac{n_+ - n_-}{2} \label{s12e32}
\end{eqnarray}
in terms of which equations \eqref{s12e28} to \eqref{s12e29} become
\begin{eqnarray}
J_+(x_{n_+,n_-}) &=& \sqrt{(j-m)(j+m+1)}\hslash
x_{n_+ + 1, n_- - 1} \label{s12e33} \\
J_-(x_{n_+,n_-}) &=& \sqrt{(j+m)(j-m+1)}\hslash
x_{n_+ - 1, n_- + 1} \label{s12e34} \\
J_z(x_{n_+,n_-}) &=& \hslash m x_{n_+, n_-}. \label{s12e35}
\end{eqnarray}
Equation \eqref{s12e27} leads to
\begin{equation}\label{s12e36}
J^2(x_{n_+,n_-}) = \hslash j(j+1)x_{n_+,n_-}.
\end{equation}
Observe the similarity with equations \eqref{s11e14}, \eqref{s10e29} and
\eqref{s10e24}.
\bibliographystyle{plain}
\bibliography{am}
\end{document}
